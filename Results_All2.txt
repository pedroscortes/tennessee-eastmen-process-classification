Random Forest -> TestAcc: 0.8494
Classification Report =
              precision    recall  f1-score   support

           0       0.41      0.57      0.48       390
           1       1.00      1.00      1.00       383
           2       1.00      0.98      0.99       383
           3       0.50      0.57      0.53       383
           4       0.93      0.99      0.96       383
           5       0.93      0.95      0.94       384
           6       1.00      1.00      1.00       383
           7       1.00      1.00      1.00       384
           8       1.00      0.95      0.97       384
           9       0.49      0.54      0.51       383
          10       0.90      0.85      0.87       383
          11       0.94      0.77      0.84       384
          12       0.96      0.95      0.95       383
          13       0.99      0.95      0.97       384
          14       0.97      0.97      0.97       383
          15       0.47      0.49      0.48       384
          16       0.91      0.79      0.84       383
          17       0.94      0.89      0.92       383
          18       0.98      0.90      0.94       383
          19       0.80      0.83      0.81       383
          20       0.93      0.77      0.85       384
          21       0.99      0.99      0.99       384

    accuracy                           0.85      8441
   macro avg       0.87      0.85      0.86      8441
weighted avg       0.86      0.85      0.86      8441

Accuracy= 84.943 %
Macro-averaged Precision = 0.865 %
Micro-averaged Precision = 0.849 %
Macro-averaged Recall = 0.850 %
Micro-averaged Recall = 0.849 %
Macro-averaged F1 = 0.856 %
Micro-averaged F1 = 0.849 %
===============
MLP -> TestAcc: 0.7937
Best parameters: {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (50, 50), 'learning_rate_init': 0.01, 'solver': 'adam'}
Classification Report =
              precision    recall  f1-score   support

           0       0.31      0.39      0.34       390
           1       0.99      1.00      1.00       383
           2       1.00      0.98      0.99       383
           3       0.32      0.29      0.31       383
           4       0.94      0.96      0.95       383
           5       1.00      0.98      0.99       384
           6       1.00      1.00      1.00       383
           7       1.00      1.00      1.00       384
           8       0.94      0.92      0.93       384
           9       0.28      0.33      0.31       383
          10       0.78      0.90      0.84       383
          11       0.82      0.76      0.79       384
          12       0.89      0.91      0.90       383
          13       0.98      0.91      0.94       384
          14       0.99      0.94      0.97       383
          15       0.29      0.18      0.22       384
          16       0.85      0.80      0.82       383
          17       0.92      0.91      0.92       383
          18       0.93      0.89      0.91       383
          19       0.77      0.90      0.83       383
          20       0.86      0.77      0.81       384
          21       0.67      0.76      0.71       384

    accuracy                           0.79      8441
   macro avg       0.80      0.79      0.79      8441
weighted avg       0.80      0.79      0.79      8441

Accuracy= 79.374 %
Macro-averaged Precision = 0.797 %
Micro-averaged Precision = 0.794 %
Macro-averaged Recall = 0.794 %
Micro-averaged Recall = 0.794 %
Macro-averaged F1 = 0.794 %
Micro-averaged F1 = 0.794 %
===============
